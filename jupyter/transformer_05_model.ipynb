{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大语言模型Transformer库-Model组件实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08bf2d38beb48c2aca57a31d417ac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1247be509844fa3b6c6854bb93d147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/156M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35791cf6dd0d4754860184bcc4060da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9afde89dd91429ea80fc587466fe1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0fde61ce3e4a9fa714adead9a64140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81aa51a2ee84329a09151c3d14df6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04efcbc30a74e568f709c493dd33846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# 在线加载\n",
    "# model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True)\n",
    "# 离线加载\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"hfl/rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定保存模型和分词器的目录路径\n",
    "model_save_path = \"path_to_save_model\"\n",
    "tokenizer_save_path = \"path_to_save_tokenizer\"\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载参数\n",
    "```bash\n",
    "BertConfig {\n",
    "  \"_name_or_path\": \"/root/代码/Model组件/rbt3\",\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"directionality\": \"bidi\",\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 3,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"pooler_fc_size\": 768,\n",
    "  \"pooler_num_attention_heads\": 12,\n",
    "  \"pooler_num_fc_layers\": 3,\n",
    "  \"pooler_size_per_head\": 128,\n",
    "  \"pooler_type\": \"first_token_transform\",\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 21128\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda/envs/transformers/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"hfl/rbt3\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  791, 1921, 1921, 3698,  679, 7231, 8024, 2769, 4638, 2552, 2658,\n",
       "          738,  679, 7231, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"今天天气不错，我的心情也不错！\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不带Model Head的模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1970,  0.6945,  1.0645,  ..., -0.0763,  0.2589, -0.5017],\n",
       "         [-0.2034,  0.7565,  0.7339,  ...,  0.0791, -0.7068,  0.2239],\n",
       "         [ 0.5075,  0.6311, -0.0454,  ..., -0.5820,  0.2189,  0.0670],\n",
       "         ...,\n",
       "         [ 0.3410,  0.2382, -0.1919,  ..., -0.0397,  0.2051,  0.0454],\n",
       "         [ 0.4606,  0.3564, -0.4006,  ..., -0.3121,  0.3912, -0.1599],\n",
       "         [ 0.1961,  0.6968,  1.0595,  ..., -0.0724,  0.2590, -0.5024]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 8.7897e-02, -9.9727e-01, -9.9996e-01, -9.4828e-01, -1.0888e-01,\n",
       "         -4.5711e-02,  5.6735e-02,  5.0818e-01,  9.9614e-01,  9.9983e-01,\n",
       "          3.4609e-02, -1.0000e+00, -4.3255e-02,  9.9985e-01, -9.9996e-01,\n",
       "          9.9992e-01,  9.3540e-01,  9.9044e-01, -9.5642e-01, -6.8368e-02,\n",
       "         -9.8755e-01, -5.2575e-01,  7.7211e-04,  9.8415e-01,  9.9882e-01,\n",
       "         -9.8423e-01, -9.9975e-01,  2.0484e-01, -6.7945e-01, -9.9991e-01,\n",
       "         -9.9850e-01, -9.9895e-01, -4.9863e-02,  7.4594e-02,  9.9662e-01,\n",
       "         -9.9026e-01, -1.7357e-01, -9.9346e-01, -9.7486e-01, -9.9817e-01,\n",
       "          3.2192e-02,  9.8479e-01, -4.5753e-02,  9.9992e-01,  4.2586e-01,\n",
       "         -1.4156e-01,  9.9992e-01,  9.9532e-01,  1.1511e-02, -9.3697e-01,\n",
       "          8.9459e-02,  5.4355e-02, -9.7445e-01,  9.8926e-01,  9.8261e-02,\n",
       "          2.6103e-01,  9.6294e-01, -9.9984e-01, -9.9975e-01,  9.9534e-01,\n",
       "         -9.9794e-01,  9.9373e-01,  9.3387e-01,  9.9375e-01, -7.5352e-01,\n",
       "          9.9910e-01,  9.9936e-01, -8.4685e-03, -3.5408e-01, -9.9998e-01,\n",
       "         -5.7192e-01, -9.7732e-01, -9.9985e-01, -2.1450e-01, -2.1986e-02,\n",
       "         -9.9587e-01,  9.9745e-01, -6.9799e-02,  9.9989e-01,  4.3815e-01,\n",
       "         -9.9963e-01, -5.8186e-02,  1.6843e-03,  2.7440e-01,  9.9927e-01,\n",
       "          9.9998e-01, -1.1338e-02, -9.9137e-01, -3.5732e-01, -9.9972e-01,\n",
       "          5.5829e-01,  9.9873e-01,  9.9999e-01, -9.9991e-01,  9.9999e-01,\n",
       "          1.3785e-01,  3.6804e-01, -9.5956e-02, -9.8797e-01,  9.3235e-01,\n",
       "         -6.5698e-02, -3.5064e-02,  9.9998e-01,  9.9802e-01,  7.0581e-01,\n",
       "         -9.9977e-01, -9.8764e-01,  9.9973e-01, -9.9963e-01, -1.0824e-01,\n",
       "          1.0000e+00,  1.8538e-01,  1.0000e+00,  9.9966e-01,  9.9961e-01,\n",
       "         -9.9982e-01, -2.9517e-01,  1.7657e-01, -9.9993e-01,  9.9834e-01,\n",
       "         -9.9221e-01,  7.2347e-01, -9.0229e-01, -1.5707e-01, -1.9675e-01,\n",
       "         -9.9999e-01,  1.5536e-01,  1.6005e-01, -9.3812e-01, -9.9912e-01,\n",
       "         -9.9963e-01, -9.9967e-01,  9.7150e-01,  9.3432e-01, -4.0527e-01,\n",
       "          3.9668e-03,  5.5284e-02,  2.4939e-02, -9.9999e-01, -9.9441e-01,\n",
       "         -9.9996e-01,  9.7594e-01, -5.7623e-01,  9.9328e-01, -9.9909e-01,\n",
       "          9.9986e-01, -9.9983e-01,  9.9979e-01,  8.9207e-01, -1.9467e-01,\n",
       "         -9.4425e-01, -9.8158e-02, -9.7924e-01, -3.8702e-02,  7.0780e-03,\n",
       "          9.8384e-01,  9.9013e-01,  4.9169e-01, -9.3678e-01,  9.9995e-01,\n",
       "         -9.4485e-01,  9.6469e-01,  4.2212e-01,  9.9902e-01,  1.0000e+00,\n",
       "         -9.9999e-01,  1.5942e-02, -1.0000e+00,  8.7981e-01,  1.2937e-01,\n",
       "          9.9992e-01,  9.9989e-01,  9.3285e-01,  9.9973e-01, -7.1220e-01,\n",
       "         -9.9831e-01,  8.8290e-01, -9.9985e-01,  9.2686e-01,  9.9997e-01,\n",
       "         -3.3997e-01,  6.5809e-01,  9.9999e-01,  7.8516e-01,  9.7941e-01,\n",
       "         -1.6566e-01, -3.3937e-01, -9.9860e-01,  2.6903e-01,  9.6710e-01,\n",
       "          9.9753e-01, -9.9669e-01, -2.0651e-02,  9.6975e-01,  1.3427e-01,\n",
       "          8.4909e-03, -9.9439e-01, -9.9107e-01,  9.9994e-01,  9.9475e-01,\n",
       "         -2.5972e-01, -2.7702e-01,  9.9995e-01, -2.7628e-01,  9.9973e-01,\n",
       "          1.1936e-02,  4.9786e-01, -2.0445e-01,  9.9994e-01,  2.4081e-01,\n",
       "          6.2504e-01,  9.3448e-02,  9.9984e-01, -9.4002e-01, -9.9976e-01,\n",
       "         -3.2386e-02,  7.1352e-01,  9.6645e-02, -9.9591e-01, -6.6644e-01,\n",
       "         -1.5138e-01,  9.9962e-01,  1.1905e-01, -9.7156e-01,  9.9794e-01,\n",
       "         -9.9936e-01,  2.9992e-01, -9.9999e-01, -9.9644e-01,  9.9997e-01,\n",
       "         -1.2528e-01, -1.0000e+00,  7.7531e-01,  9.9996e-01, -9.5558e-01,\n",
       "          9.9896e-01, -3.7476e-01, -9.9986e-01,  9.6238e-02,  7.4137e-02,\n",
       "         -1.0000e+00, -9.9512e-01, -1.0000e+00, -3.0854e-01, -2.6146e-02,\n",
       "          9.9654e-01, -1.0000e+00, -1.7391e-01, -1.0000e+00,  9.9578e-01,\n",
       "          9.9988e-01, -7.2861e-02,  4.0041e-01,  9.9997e-01,  6.7367e-01,\n",
       "          5.1275e-02, -3.3652e-01,  2.9073e-01,  1.7156e-01,  9.8787e-01,\n",
       "          2.3903e-01,  9.9909e-01, -9.9974e-01,  5.0375e-01,  2.8485e-01,\n",
       "         -9.9998e-01,  1.9863e-01,  9.8562e-01,  4.3613e-02,  3.7736e-02,\n",
       "         -3.1426e-02,  3.5094e-02,  9.6066e-01,  9.9918e-01,  1.0000e+00,\n",
       "          9.8407e-01,  1.0000e+00,  1.0000e+00,  3.3744e-02, -9.9328e-01,\n",
       "         -1.2667e-01,  1.7480e-01, -9.9996e-01, -9.9675e-01, -9.9685e-01,\n",
       "          7.7465e-01, -6.0408e-02,  1.0000e+00, -1.0000e+00,  1.0000e+00,\n",
       "         -8.3424e-01, -9.5948e-03,  6.5462e-02, -3.5990e-01, -8.6003e-01,\n",
       "         -1.4320e-01,  9.5629e-01,  7.9050e-03,  9.9146e-01,  9.8961e-01,\n",
       "          1.1626e-01, -1.0432e-01,  3.5564e-01,  3.2014e-02,  9.7454e-01,\n",
       "         -9.8932e-01,  9.8897e-01,  1.1887e-01,  9.8188e-01, -5.4718e-01,\n",
       "         -9.9999e-01,  9.9890e-01, -9.9970e-01, -3.0678e-01, -9.9962e-01,\n",
       "         -9.9364e-01, -2.9203e-04, -9.8216e-01,  5.7141e-01,  9.9705e-01,\n",
       "         -1.9593e-01, -5.7650e-01, -9.9993e-01, -7.4781e-01,  9.9620e-01,\n",
       "          9.9968e-01, -9.9992e-01,  9.9783e-01,  9.6304e-01, -9.7966e-01,\n",
       "          2.8709e-01,  9.9427e-01, -9.9402e-01,  9.9470e-01, -9.9991e-01,\n",
       "         -1.7627e-01,  9.9723e-01,  1.6608e-01, -9.9996e-01, -9.2648e-01,\n",
       "         -9.1951e-02, -2.0002e-01, -1.9851e-01,  9.9995e-01, -1.4904e-01,\n",
       "         -3.3734e-01, -9.9995e-01, -9.8681e-01, -8.4735e-01,  7.0820e-02,\n",
       "          9.9505e-01, -9.9980e-01,  4.9846e-02,  9.6678e-01, -9.9254e-01,\n",
       "         -2.0599e-01,  2.2419e-01, -6.4226e-02, -2.6558e-01, -9.9616e-01,\n",
       "         -9.9907e-01, -9.9963e-01,  9.9984e-01, -2.2054e-01,  3.1947e-01,\n",
       "          9.9897e-01,  9.9999e-01,  9.9980e-01, -9.1570e-01,  6.6829e-01,\n",
       "          9.9935e-01,  1.4527e-01, -2.8419e-01, -9.8884e-01,  1.2444e-01,\n",
       "         -9.1105e-01, -9.5433e-01,  1.0701e-01,  6.9387e-01,  3.5031e-01,\n",
       "         -9.6053e-01,  9.9999e-01,  9.9840e-01,  1.0000e+00,  4.5313e-02,\n",
       "         -7.6507e-01,  6.5702e-01, -7.2056e-01, -9.9957e-01, -5.2287e-02,\n",
       "          9.9952e-01, -9.9652e-01,  1.4545e-01,  4.6641e-01, -9.8142e-01,\n",
       "          9.9981e-01, -9.9195e-01, -6.6828e-02, -1.0000e+00, -9.9931e-01,\n",
       "         -1.0000e+00,  9.9999e-01,  1.0356e-01,  1.4495e-01, -9.9319e-01,\n",
       "          9.9999e-01,  8.9095e-01, -7.8199e-01,  3.5330e-01,  9.9982e-01,\n",
       "         -2.6741e-01, -8.3254e-01, -1.0000e+00, -9.9920e-01,  9.9605e-01,\n",
       "         -1.7082e-01,  9.9961e-01,  9.9592e-01, -9.9502e-01,  9.4591e-01,\n",
       "          9.4074e-01,  2.8233e-01, -9.9813e-01, -9.9739e-01,  5.9736e-02,\n",
       "          2.2457e-01,  4.2030e-02, -1.6268e-01, -4.9971e-02,  9.9976e-01,\n",
       "         -1.6776e-01, -2.2165e-01,  2.9483e-01,  9.9999e-01,  9.6379e-01,\n",
       "         -9.9906e-01, -5.2803e-01,  6.5031e-02, -9.7005e-01, -9.9633e-01,\n",
       "         -9.9916e-01,  4.7664e-02, -8.0782e-02, -6.3072e-02, -9.8359e-01,\n",
       "         -9.9981e-01, -9.9791e-01, -2.2112e-01, -9.9656e-01, -9.9841e-01,\n",
       "         -3.8548e-02, -9.9984e-01, -9.9874e-01,  9.9995e-01, -9.9987e-01,\n",
       "          3.0959e-01,  7.9757e-01,  9.6888e-01, -9.9979e-01, -2.6795e-01,\n",
       "          9.8794e-01, -9.9855e-01,  2.2833e-01, -9.9554e-01,  2.0067e-01,\n",
       "         -9.4325e-01, -9.9999e-01,  6.2175e-01,  9.9986e-01,  9.9925e-01,\n",
       "          9.9935e-01,  9.4470e-01, -8.1409e-01,  7.3553e-01,  9.9930e-01,\n",
       "          9.9997e-01, -4.3103e-01, -3.1709e-01, -1.0000e+00, -6.8292e-01,\n",
       "          8.0976e-01, -4.4156e-02,  8.0144e-01, -9.9989e-01, -3.6913e-02,\n",
       "         -8.9215e-01,  3.4739e-01,  1.0000e+00,  9.9135e-01,  7.7215e-01,\n",
       "         -1.0000e+00,  3.7421e-01, -4.2161e-01, -1.1871e-01, -9.1360e-01,\n",
       "          7.6272e-02,  9.9997e-01, -9.8893e-01, -5.4643e-02, -9.8828e-01,\n",
       "         -9.9949e-01,  1.0000e+00, -9.9996e-01,  9.9989e-01,  8.6105e-01,\n",
       "         -9.9277e-01,  1.5934e-01, -8.5729e-01, -9.8352e-03, -1.1422e-01,\n",
       "         -1.6463e-01, -3.1912e-01,  3.4700e-01, -9.9997e-01,  2.8660e-01,\n",
       "          9.9912e-01, -9.0442e-02, -8.2121e-01, -9.9952e-01,  1.3222e-01,\n",
       "          9.9020e-01, -9.9942e-01, -9.9989e-01, -3.1020e-01, -3.0586e-01,\n",
       "          2.1598e-02, -2.8270e-01, -1.6540e-01,  2.5044e-01, -9.6322e-01,\n",
       "          1.2396e-01,  9.3931e-01, -8.7046e-01,  9.6460e-01, -8.1961e-01,\n",
       "         -9.7975e-01, -3.7829e-01,  9.9982e-01,  9.9972e-01, -9.9994e-01,\n",
       "         -9.9987e-01, -6.9176e-01,  7.1783e-02, -9.1331e-01,  9.9793e-01,\n",
       "         -3.0751e-01, -9.8649e-01,  5.9018e-02,  1.9527e-01, -2.5044e-01,\n",
       "         -9.3673e-01,  1.0000e+00, -9.9000e-01,  1.0000e+00, -9.9999e-01,\n",
       "         -9.9627e-01,  3.2593e-01,  9.9987e-01, -9.9997e-01,  4.3372e-03,\n",
       "          9.9989e-01, -9.9999e-01, -1.6702e-01, -9.1441e-01,  8.2956e-01,\n",
       "         -2.3889e-01, -1.6671e-01,  7.4933e-01, -9.9809e-01,  9.4500e-02,\n",
       "         -9.9959e-01,  9.3608e-01,  9.9644e-01, -9.9864e-01,  3.0222e-01,\n",
       "         -9.9999e-01,  6.1069e-03, -1.0355e-01, -9.9982e-01,  9.9793e-01,\n",
       "          9.9993e-01, -1.0849e-01,  5.5703e-02, -9.9938e-01, -2.8637e-01,\n",
       "          2.6118e-02, -9.9924e-01,  2.6675e-01, -9.9991e-01,  7.9626e-01,\n",
       "         -9.9134e-01,  9.9739e-01, -9.9989e-01,  9.7733e-01,  9.7881e-01,\n",
       "         -2.4788e-01, -9.8400e-01, -1.1093e-01,  4.0175e-01, -9.9988e-01,\n",
       "         -1.4240e-01, -9.9972e-01, -9.9776e-01,  2.6337e-01,  9.9809e-01,\n",
       "          9.9300e-01,  2.7149e-01,  9.9517e-01, -9.9046e-01, -1.2484e-01,\n",
       "         -1.4576e-01,  5.5871e-01,  1.0000e+00, -9.9974e-01, -9.9999e-01,\n",
       "          9.2859e-01, -9.9997e-01, -9.9682e-01,  1.0000e+00,  1.6686e-01,\n",
       "          9.9989e-01,  3.2883e-02, -9.9054e-01,  1.9755e-01,  1.1448e-01,\n",
       "          9.9978e-01,  3.1224e-01, -6.5309e-02,  9.9761e-01,  9.5122e-02,\n",
       "          4.0933e-01, -5.1634e-01,  9.8577e-01, -1.8362e-01, -2.5112e-01,\n",
       "          9.5914e-01, -9.7617e-01, -9.9934e-01, -9.9726e-01,  3.2479e-01,\n",
       "         -3.4380e-02,  1.4588e-01,  2.5041e-01,  8.9577e-01,  9.9960e-01,\n",
       "         -9.9765e-01,  9.9691e-01, -1.0000e+00, -9.9990e-01, -1.7630e-01,\n",
       "          2.1974e-01,  9.5018e-01,  2.2983e-01, -6.4162e-01,  2.8954e-02,\n",
       "         -9.9931e-01,  9.4642e-01, -9.9286e-01,  9.9836e-01,  1.6662e-01,\n",
       "          1.3261e-01,  9.9999e-01,  9.9891e-01, -1.4025e-02, -9.9733e-01,\n",
       "         -7.2127e-01, -4.4097e-02, -9.9925e-01,  9.9776e-01,  1.9446e-01,\n",
       "         -1.0345e-02,  2.8346e-01,  1.0380e-01, -9.9916e-01, -9.7873e-01,\n",
       "         -4.9284e-03,  9.9902e-01, -9.9951e-01,  9.8975e-01, -9.7011e-01,\n",
       "          9.9642e-01,  9.9948e-01,  1.0000e+00,  5.4173e-02,  9.9116e-01,\n",
       "         -9.9969e-01, -9.9237e-01,  9.9101e-01,  9.9956e-01,  9.9999e-01,\n",
       "          9.9412e-01,  9.4576e-01,  2.2004e-01, -9.9980e-01,  9.9660e-01,\n",
       "         -1.0824e-01,  1.0936e-01,  1.6928e-01, -9.5593e-01, -9.9996e-01,\n",
       "          9.9998e-01, -1.0000e+00, -9.9950e-01, -9.5445e-01, -9.9981e-01,\n",
       "          9.9858e-01,  7.6530e-01,  9.7716e-01,  2.5118e-01, -9.9948e-01,\n",
       "         -9.8890e-01,  1.6850e-01, -9.4929e-01, -8.7788e-01, -6.4734e-02,\n",
       "         -1.0000e+00,  1.3314e-01,  1.2207e-01, -9.8800e-01, -4.5001e-01,\n",
       "         -6.1903e-01,  4.5318e-01,  9.8663e-01, -7.3643e-01,  8.6084e-01,\n",
       "         -9.8826e-01, -9.9926e-01,  8.3206e-02, -9.9999e-01,  9.9089e-01,\n",
       "          9.9864e-01, -9.1663e-02,  5.7305e-01, -8.8494e-01,  3.7489e-01,\n",
       "         -9.9996e-01, -1.0000e+00,  9.7592e-01,  9.9989e-01, -2.6412e-01,\n",
       "         -9.8590e-01, -2.6750e-01, -9.9668e-01, -2.4275e-01,  9.5011e-01,\n",
       "          9.9968e-01, -9.9552e-01,  9.9527e-01, -9.7998e-01,  1.0432e-01,\n",
       "          9.9678e-01, -1.0000e+00,  9.0447e-01, -9.9949e-01,  9.9991e-01,\n",
       "         -1.0000e+00,  9.9917e-01, -1.8667e-01,  1.6859e-01, -6.7974e-02,\n",
       "          8.3043e-01, -9.9982e-01, -4.6980e-02,  8.6840e-01,  9.9824e-01,\n",
       "          5.7123e-02,  9.9040e-01,  1.8548e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[4.3597e-01, 5.9649e-04, 2.1022e-04,  ..., 3.2442e-04,\n",
       "           4.7392e-04, 5.5803e-01],\n",
       "          [6.1595e-03, 6.1577e-02, 4.1553e-02,  ..., 1.0155e-01,\n",
       "           1.6889e-01, 3.6948e-03],\n",
       "          [9.1790e-03, 5.1454e-02, 1.1191e-01,  ..., 6.6033e-02,\n",
       "           1.0489e-01, 2.6063e-03],\n",
       "          ...,\n",
       "          [1.2646e-02, 6.5414e-02, 4.1082e-02,  ..., 6.7520e-02,\n",
       "           2.3407e-01, 7.9361e-03],\n",
       "          [3.3759e-02, 8.3109e-02, 2.6811e-02,  ..., 3.3384e-02,\n",
       "           2.6509e-01, 2.0343e-02],\n",
       "          [3.8062e-01, 1.4764e-03, 5.1050e-04,  ..., 7.5240e-04,\n",
       "           1.1861e-03, 6.0995e-01]],\n",
       "\n",
       "         [[9.9248e-01, 2.6609e-05, 1.8100e-04,  ..., 1.7125e-05,\n",
       "           5.6677e-04, 1.9198e-03],\n",
       "          [7.5085e-01, 9.2232e-05, 2.4623e-01,  ..., 6.0214e-08,\n",
       "           1.5332e-04, 4.6545e-04],\n",
       "          [1.0539e-02, 7.5684e-01, 2.5290e-04,  ..., 5.3240e-08,\n",
       "           1.2264e-07, 1.5530e-05],\n",
       "          ...,\n",
       "          [3.0746e-02, 4.3140e-09, 2.3041e-07,  ..., 4.7650e-05,\n",
       "           5.3159e-03, 9.4940e-04],\n",
       "          [2.0016e-01, 2.4902e-06, 1.0353e-06,  ..., 1.0441e-03,\n",
       "           3.4001e-04, 7.9825e-01],\n",
       "          [5.8139e-01, 1.6971e-06, 5.1742e-05,  ..., 6.1539e-05,\n",
       "           4.1333e-01, 2.2730e-03]],\n",
       "\n",
       "         [[1.6328e-01, 8.1107e-02, 4.3542e-02,  ..., 1.9741e-02,\n",
       "           5.2629e-02, 1.9114e-01],\n",
       "          [5.9557e-01, 1.6230e-01, 2.8149e-02,  ..., 4.5337e-03,\n",
       "           3.1953e-02, 9.5919e-02],\n",
       "          [4.5090e-01, 7.7780e-02, 2.0875e-01,  ..., 3.0667e-03,\n",
       "           1.8378e-02, 8.3858e-02],\n",
       "          ...,\n",
       "          [7.9327e-02, 2.7123e-02, 9.5639e-03,  ..., 6.1642e-02,\n",
       "           2.6690e-02, 2.5611e-02],\n",
       "          [3.8873e-02, 2.0976e-02, 1.2524e-02,  ..., 3.4304e-02,\n",
       "           7.1615e-02, 7.2250e-02],\n",
       "          [3.0149e-02, 2.6212e-02, 1.4725e-02,  ..., 5.8573e-02,\n",
       "           1.3303e-01, 1.0729e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[7.0954e-01, 1.6721e-02, 9.7419e-03,  ..., 1.2647e-02,\n",
       "           2.3923e-02, 9.0965e-02],\n",
       "          [3.2820e-01, 4.9856e-02, 2.0844e-02,  ..., 1.6804e-02,\n",
       "           3.6384e-02, 1.7270e-01],\n",
       "          [3.2339e-01, 7.0307e-03, 1.3158e-01,  ..., 1.7156e-03,\n",
       "           3.4726e-02, 8.4886e-02],\n",
       "          ...,\n",
       "          [1.8427e-01, 8.9827e-03, 3.2907e-03,  ..., 1.2251e-01,\n",
       "           4.0050e-03, 1.3400e-02],\n",
       "          [8.4342e-02, 1.0265e-01, 3.4778e-02,  ..., 2.2651e-02,\n",
       "           2.7575e-01, 2.3388e-01],\n",
       "          [3.1833e-01, 4.0528e-02, 3.5696e-02,  ..., 2.3703e-02,\n",
       "           1.0863e-02, 1.2260e-01]],\n",
       "\n",
       "         [[9.7134e-01, 3.6497e-03, 2.5290e-03,  ..., 1.4400e-03,\n",
       "           1.0024e-03, 1.8071e-03],\n",
       "          [7.5845e-03, 6.9435e-02, 7.1800e-01,  ..., 3.2620e-04,\n",
       "           3.6132e-03, 7.7624e-03],\n",
       "          [3.3012e-03, 5.4443e-03, 4.4530e-02,  ..., 3.3695e-05,\n",
       "           8.1195e-05, 2.6429e-03],\n",
       "          ...,\n",
       "          [4.4712e-03, 5.6174e-04, 1.1346e-03,  ..., 1.6499e-02,\n",
       "           8.2970e-01, 1.3724e-01],\n",
       "          [1.8237e-02, 1.1016e-03, 4.1965e-04,  ..., 9.9762e-03,\n",
       "           9.1209e-02, 8.7032e-01],\n",
       "          [9.9387e-01, 2.5598e-04, 1.6384e-04,  ..., 5.7160e-04,\n",
       "           6.3076e-04, 4.0173e-03]],\n",
       "\n",
       "         [[4.4069e-01, 6.9305e-02, 1.9732e-02,  ..., 1.3711e-02,\n",
       "           2.1639e-02, 1.9365e-01],\n",
       "          [6.9933e-01, 4.3897e-02, 2.5569e-02,  ..., 4.7080e-03,\n",
       "           1.3597e-02, 3.8648e-02],\n",
       "          [3.0555e-02, 9.4718e-01, 9.5704e-03,  ..., 4.8997e-04,\n",
       "           2.5337e-04, 1.7300e-03],\n",
       "          ...,\n",
       "          [2.6853e-02, 5.1967e-05, 7.0294e-05,  ..., 2.0356e-02,\n",
       "           2.1908e-02, 1.2942e-02],\n",
       "          [2.5048e-01, 6.6332e-03, 2.4812e-04,  ..., 5.1478e-01,\n",
       "           8.4713e-02, 3.5747e-02],\n",
       "          [8.3051e-02, 1.9542e-03, 1.2336e-03,  ..., 3.8227e-02,\n",
       "           3.6866e-01, 4.7605e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.2730e-01, 3.0275e-03, 5.1221e-03,  ..., 5.2739e-03,\n",
       "           4.7576e-02, 4.1696e-01],\n",
       "          [4.6647e-01, 6.2594e-03, 2.0219e-02,  ..., 4.6308e-05,\n",
       "           2.5371e-04, 4.9702e-01],\n",
       "          [8.5384e-02, 8.3061e-01, 1.2188e-04,  ..., 4.9624e-05,\n",
       "           8.6715e-05, 8.2800e-02],\n",
       "          ...,\n",
       "          [2.7181e-01, 1.6402e-06, 3.3655e-07,  ..., 6.5533e-04,\n",
       "           1.7290e-02, 2.6980e-01],\n",
       "          [2.2597e-01, 4.6329e-03, 2.2462e-04,  ..., 5.0493e-01,\n",
       "           1.9603e-02, 2.2825e-01],\n",
       "          [4.2763e-01, 3.0512e-03, 5.2534e-03,  ..., 5.0890e-03,\n",
       "           4.9113e-02, 4.1711e-01]],\n",
       "\n",
       "         [[4.6560e-01, 1.2746e-02, 6.7410e-03,  ..., 6.5750e-03,\n",
       "           8.9664e-03, 4.4864e-01],\n",
       "          [7.0488e-02, 1.1836e-01, 4.2986e-02,  ..., 3.5898e-02,\n",
       "           1.2873e-01, 7.1667e-02],\n",
       "          [5.6974e-02, 8.5027e-02, 4.0045e-02,  ..., 5.1108e-02,\n",
       "           1.4868e-01, 5.8337e-02],\n",
       "          ...,\n",
       "          [1.8981e-01, 3.1292e-02, 1.3202e-02,  ..., 6.3148e-02,\n",
       "           2.5480e-01, 1.8980e-01],\n",
       "          [1.2370e-01, 2.7470e-02, 1.8424e-02,  ..., 7.1212e-02,\n",
       "           2.6336e-01, 1.2664e-01],\n",
       "          [4.6726e-01, 1.2001e-02, 6.5265e-03,  ..., 6.3153e-03,\n",
       "           8.5349e-03, 4.5069e-01]],\n",
       "\n",
       "         [[4.7071e-01, 1.3079e-02, 1.3435e-02,  ..., 5.3749e-03,\n",
       "           5.9321e-03, 4.3184e-01],\n",
       "          [4.3146e-01, 1.2550e-01, 1.0165e-02,  ..., 1.0051e-04,\n",
       "           1.2925e-03, 4.2085e-01],\n",
       "          [4.0427e-01, 4.8907e-02, 1.0858e-01,  ..., 6.8906e-05,\n",
       "           1.2086e-03, 3.9309e-01],\n",
       "          ...,\n",
       "          [6.4197e-02, 5.5042e-05, 2.4966e-05,  ..., 2.2156e-01,\n",
       "           1.8238e-03, 6.2064e-02],\n",
       "          [3.4499e-01, 6.8315e-04, 6.4348e-04,  ..., 5.7364e-03,\n",
       "           2.6762e-01, 3.3075e-01],\n",
       "          [4.6918e-01, 1.3613e-02, 1.3977e-02,  ..., 5.6614e-03,\n",
       "           5.9920e-03, 4.3004e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.5201e-01, 7.4343e-03, 5.0779e-03,  ..., 7.8672e-03,\n",
       "           1.8971e-02, 4.2773e-01],\n",
       "          [1.3923e-01, 3.2310e-02, 3.2213e-02,  ..., 3.0662e-03,\n",
       "           8.4442e-03, 1.3901e-01],\n",
       "          [1.1229e-01, 7.8657e-03, 2.2069e-02,  ..., 1.7291e-03,\n",
       "           4.4762e-03, 1.1245e-01],\n",
       "          ...,\n",
       "          [4.1118e-01, 5.0321e-03, 2.7606e-03,  ..., 3.4085e-02,\n",
       "           8.6255e-02, 4.2300e-01],\n",
       "          [4.5969e-01, 5.5336e-03, 4.6755e-03,  ..., 6.1327e-03,\n",
       "           2.5411e-02, 4.6015e-01],\n",
       "          [4.5215e-01, 7.4504e-03, 5.1742e-03,  ..., 7.5876e-03,\n",
       "           1.8597e-02, 4.2831e-01]],\n",
       "\n",
       "         [[4.8294e-01, 8.0974e-03, 4.6455e-03,  ..., 2.0706e-03,\n",
       "           5.3353e-03, 4.5900e-01],\n",
       "          [2.5496e-01, 8.8459e-02, 3.1862e-02,  ..., 1.6392e-02,\n",
       "           3.8127e-02, 2.5294e-01],\n",
       "          [2.1885e-01, 1.2608e-01, 4.0218e-02,  ..., 1.2497e-02,\n",
       "           4.5266e-02, 2.1476e-01],\n",
       "          ...,\n",
       "          [7.4709e-02, 4.9971e-02, 1.6085e-02,  ..., 4.7474e-03,\n",
       "           3.7517e-02, 7.3507e-02],\n",
       "          [8.0010e-02, 1.6857e-01, 2.8056e-02,  ..., 1.0664e-02,\n",
       "           5.9267e-02, 8.1712e-02],\n",
       "          [4.8205e-01, 8.2513e-03, 4.8037e-03,  ..., 2.0992e-03,\n",
       "           5.3701e-03, 4.5869e-01]],\n",
       "\n",
       "         [[4.5781e-01, 1.0651e-02, 5.8958e-03,  ..., 1.3913e-03,\n",
       "           9.9142e-03, 4.4547e-01],\n",
       "          [2.7243e-01, 1.5408e-01, 4.6743e-02,  ..., 7.0851e-03,\n",
       "           1.6408e-02, 2.6628e-01],\n",
       "          [2.0099e-01, 8.1235e-02, 3.0676e-02,  ..., 2.9880e-03,\n",
       "           8.4443e-03, 1.9683e-01],\n",
       "          ...,\n",
       "          [3.1651e-01, 9.6433e-03, 8.2255e-03,  ..., 8.5355e-02,\n",
       "           1.4456e-02, 3.0911e-01],\n",
       "          [2.9976e-01, 7.4528e-02, 2.5535e-02,  ..., 2.3070e-02,\n",
       "           3.0245e-02, 2.9621e-01],\n",
       "          [4.5661e-01, 1.0737e-02, 6.0507e-03,  ..., 1.3996e-03,\n",
       "           9.9401e-03, 4.4458e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.0549e-01, 4.2258e-02, 2.4484e-02,  ..., 4.4828e-02,\n",
       "           9.0527e-02, 2.0252e-01],\n",
       "          [3.3543e-01, 1.2777e-01, 4.3113e-02,  ..., 4.1651e-03,\n",
       "           2.4823e-02, 3.3239e-01],\n",
       "          [2.8512e-01, 1.9035e-01, 7.4707e-02,  ..., 2.0296e-03,\n",
       "           2.0780e-02, 2.8261e-01],\n",
       "          ...,\n",
       "          [3.2667e-01, 7.2090e-03, 5.3815e-03,  ..., 7.1504e-03,\n",
       "           3.0044e-03, 3.2303e-01],\n",
       "          [2.1450e-01, 2.1286e-02, 1.8367e-02,  ..., 2.0196e-02,\n",
       "           4.9758e-02, 2.1211e-01],\n",
       "          [2.0648e-01, 4.1904e-02, 2.4352e-02,  ..., 4.4628e-02,\n",
       "           9.0227e-02, 2.0350e-01]],\n",
       "\n",
       "         [[1.1530e-02, 2.4392e-01, 1.2182e-01,  ..., 4.6202e-03,\n",
       "           1.1124e-01, 1.1445e-02],\n",
       "          [1.6097e-01, 8.0431e-02, 5.6991e-01,  ..., 1.3146e-04,\n",
       "           2.7862e-03, 1.5874e-01],\n",
       "          [9.7806e-02, 7.2246e-01, 4.5646e-02,  ..., 7.8335e-06,\n",
       "           1.3395e-04, 9.6154e-02],\n",
       "          ...,\n",
       "          [3.4458e-01, 1.0137e-03, 6.6256e-04,  ..., 2.1010e-02,\n",
       "           7.6586e-03, 3.4011e-01],\n",
       "          [2.3210e-01, 1.3423e-01, 2.8136e-02,  ..., 1.2647e-01,\n",
       "           4.8411e-02, 2.2898e-01],\n",
       "          [1.1501e-02, 2.4535e-01, 1.2141e-01,  ..., 4.6112e-03,\n",
       "           1.1054e-01, 1.1416e-02]],\n",
       "\n",
       "         [[4.0149e-01, 2.0697e-03, 9.3420e-04,  ..., 8.7949e-04,\n",
       "           1.0412e-01, 3.9861e-01],\n",
       "          [2.9144e-01, 1.0665e-01, 1.0471e-01,  ..., 1.8553e-03,\n",
       "           1.4475e-02, 2.8796e-01],\n",
       "          [1.7032e-01, 8.4213e-02, 3.4957e-01,  ..., 1.0438e-03,\n",
       "           8.9102e-03, 1.6790e-01],\n",
       "          ...,\n",
       "          [4.2092e-01, 3.0834e-03, 5.3220e-03,  ..., 1.6086e-02,\n",
       "           2.3004e-03, 4.1847e-01],\n",
       "          [3.7852e-01, 7.7449e-03, 8.4256e-03,  ..., 9.8870e-03,\n",
       "           3.8494e-02, 3.7534e-01],\n",
       "          [4.0177e-01, 2.0748e-03, 9.3785e-04,  ..., 8.8769e-04,\n",
       "           1.0392e-01, 3.9890e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.2779e-02, 1.9341e-01, 1.0024e-01,  ..., 1.7324e-02,\n",
       "           7.2842e-02, 5.2155e-02],\n",
       "          [1.7097e-01, 4.8636e-02, 7.3367e-02,  ..., 4.6602e-04,\n",
       "           2.0411e-03, 1.6894e-01],\n",
       "          [1.9770e-01, 1.7608e-02, 2.1930e-02,  ..., 6.0065e-04,\n",
       "           1.4141e-03, 1.9485e-01],\n",
       "          ...,\n",
       "          [4.9794e-01, 1.1564e-04, 6.4593e-05,  ..., 1.8118e-03,\n",
       "           2.6637e-03, 4.9425e-01],\n",
       "          [4.8103e-01, 7.0776e-03, 4.2110e-03,  ..., 3.0849e-03,\n",
       "           1.2109e-02, 4.7883e-01],\n",
       "          [5.3071e-02, 1.9360e-01, 1.0031e-01,  ..., 1.7147e-02,\n",
       "           7.2554e-02, 5.2445e-02]],\n",
       "\n",
       "         [[2.1880e-02, 8.5664e-02, 5.0421e-02,  ..., 5.2597e-02,\n",
       "           1.4741e-01, 2.1802e-02],\n",
       "          [4.8351e-01, 2.5390e-02, 4.4586e-03,  ..., 5.6332e-05,\n",
       "           2.9283e-03, 4.7855e-01],\n",
       "          [3.8638e-01, 1.8403e-01, 2.5062e-02,  ..., 1.2949e-04,\n",
       "           2.4168e-03, 3.8372e-01],\n",
       "          ...,\n",
       "          [3.0370e-01, 1.2459e-04, 1.7634e-04,  ..., 9.5646e-03,\n",
       "           2.7993e-03, 2.9731e-01],\n",
       "          [3.0182e-01, 1.9115e-03, 7.8158e-04,  ..., 3.3202e-02,\n",
       "           3.8398e-02, 2.9676e-01],\n",
       "          [2.1936e-02, 8.5442e-02, 5.0387e-02,  ..., 5.2757e-02,\n",
       "           1.4786e-01, 2.1857e-02]],\n",
       "\n",
       "         [[1.4255e-01, 8.5803e-02, 7.1462e-02,  ..., 3.4688e-02,\n",
       "           1.2410e-01, 1.3960e-01],\n",
       "          [4.1910e-01, 1.8537e-02, 7.5835e-02,  ..., 2.3646e-04,\n",
       "           5.0823e-03, 4.1626e-01],\n",
       "          [3.9532e-01, 2.2690e-02, 5.3705e-02,  ..., 8.3117e-05,\n",
       "           5.8201e-04, 3.8996e-01],\n",
       "          ...,\n",
       "          [4.8635e-01, 5.8366e-05, 9.4799e-05,  ..., 3.5430e-03,\n",
       "           1.9444e-02, 4.8492e-01],\n",
       "          [4.4174e-01, 2.7708e-02, 1.5197e-02,  ..., 2.5591e-02,\n",
       "           1.7493e-02, 4.4098e-01],\n",
       "          [1.4294e-01, 8.6216e-02, 7.1363e-02,  ..., 3.4465e-02,\n",
       "           1.2380e-01, 1.3999e-01]]]], grad_fn=<SoftmaxBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载时设置了output_attentions=True。\n",
    "# 这意味着模型在推理时不仅会输出最终的logits或预测结果，还会输出每个层的注意力权重。\n",
    "# 这通常用于调试或分析模型的内部工作机制\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\", output_attentions=True)\n",
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带Model Head的模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5211, -0.2505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "# 使用AutoModelForSequenceClassification类加载一个用于序列分类任务的预训练模型。\n",
    "# 这个类是专门用于文本分类任务的，如情感分析或主题分类，它期望输出是整个输入序列的分类结果。\n",
    "# 它从指定路径加载模型，并使用inputs进行推理。\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "clz_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 这段代码与上面代码类似，但它在加载模型时额外指定了num_labels=2参数。\n",
    "# 这个参数对于序列分类模型是重要的，因为它告诉模型输出层期望的输出尺寸，即分类标签的数量。\n",
    "# 如果模型被微调用于一个具有两个标签的分类任务（如正面情感和负面情感），这个参数是必须的。\n",
    "# 如果没有正确设置num_labels，模型的输出可能无法正确映射到标签空间。\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
