{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大语言模型Transformer库-Trainer组件实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于Trainer优化预训练代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤一：导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "# 使用filter方法对数据集进行过滤，以移除那些\"review\"字段缺失的样本。\n",
    "# lambda x: x[\"review\"] is not None是一个匿名函数，它对于数据集中的每个样本x进行评估，\n",
    "# 如果样本中的\"review\"字段不是None（即不是缺失或空的），那么这个样本就会通过过滤。\n",
    "# 过滤后，dataset对象将只包含\"review\"字段有有效值的样本。\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤三：划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤四：数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/transformers/classmodel/rbt3\")\n",
    "# 定义一个处理函数，该函数将被应用到数据集中的每个样本上。\n",
    "# 这个函数接收一个examples对象，其中包含了数据集中的一部分样本。\n",
    "def process_function(examples):\n",
    "    # 使用tokenizer对\"review\"字段进行编码，生成模型可理解的格式。\n",
    "    # max_length=128指定了输出序列的最大长度，超过这个长度的文本将被截断。\n",
    "    # truncation=True指示Tokenizer进行截断操作。\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "# 使用map方法将process_function应用到datasets对象中的每个样本上。\n",
    "# map方法会自动对数据集中的所有样本运行process_function函数。\n",
    "# batched=True指示map操作应该批处理样本，这通常比逐个处理样本更高效。\n",
    "# remove_columns=datasets[\"train\"].column_names指定在映射过程中移除原始数据集中的列，只保留处理后的tokenized数据。\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤五：创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/root/pretrains/rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤六：创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metirc = evaluate.load(\"f1\")\n",
    "\n",
    "def eval_metric(eval_predict):\n",
    "    #eval_predict拆分成两部分，第一部分是预测值，赋值给predictions，第二部分是实际标签，赋值给labels\n",
    "    predictions, labels = eval_predict\n",
    "    # 获取到预测值\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    #是更新准确率指标，把F1分数的计算结果加入到准确率指标的计算中\n",
    "    acc.update(f1)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤七：创建TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=64,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=128,  # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型\n",
    "train_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤八：创建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  train_dataset=tokenized_datasets[\"train\"], \n",
    "                  eval_dataset=tokenized_datasets[\"test\"], \n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤九：模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤十：模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤十一：模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"csv\", data_files=\"/root/transformers/classdata/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# 加载预训练模型的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/transformers/classmodel/rbt3\")\n",
    "\n",
    "# 定义处理函数，将文本转换为模型可接受的格式\n",
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "# 对数据集进行预处理，并返回处理后的结果\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/root/transformers/classmodel/rbt3\")\n",
    "\n",
    "# 加载评估指标\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metirc = evaluate.load(\"f1\")\n",
    "\n",
    "# 定义评估函数，计算准确率和F1分数\n",
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc\n",
    "\n",
    "# 设置训练参数\n",
    "train_args = TrainingArguments(output_dir=\"./checkpoints\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=64,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=128,  # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型\n",
    "\n",
    "# 创建Trainer对象，用于训练和评估模型\n",
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  train_dataset=tokenized_datasets[\"train\"], \n",
    "                  eval_dataset=tokenized_datasets[\"test\"], \n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)\n",
    "\n",
    "# 开始训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 在测试集上评估模型性能\n",
    "trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "trainer.predict(tokenized_datasets[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
