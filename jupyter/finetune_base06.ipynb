{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型Transformer库-Datasets组件实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、加载和保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、加载数据集任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_dataset = load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、加载划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用train关键字加载训练数据集\n",
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按区间加载训练数据集\n",
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[10:100]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按百分比加载数据集\n",
    "dataset = load_dataset(\"madao33/new-title-chinese\", split=\"train[:50%]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载名为\"madao33/new-title-chinese\"的数据集，并将其划分为训练集的前50%和后50%\n",
    "datasets = load_dataset(\"madao33/new-title-chinese\", split=[\"train[:50%]\", \"train[50%:]\"])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、加载本地数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 从当前目录下的\"./processed_data\"文件夹中加载数据集，并将其赋值给变量processed_datasets。\n",
    "processed_datasets = load_from_disk(\"./processed_data\")\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files参数指定了数据集的文件路径，\n",
    "# split参数指定了要加载的数据集的划分方式（这里是\"train\"）。\n",
    "dataset = load_dataset(\"csv\", data_files=\"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_csv(\"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\")\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义List加载数据集\n",
    "# List格式的数据需要内嵌{}，明确数据字段\n",
    "data = [{\"text\": \"abc\"}, {\"text\": \"def\"}]\n",
    "# data = [\"abc\", \"def\"]\n",
    "Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用load_dataset函数从指定的JSON文件中加载数据集，并将其赋值给变量dataset。\n",
    "# 其中，field参数指定了要加载的数据集的字段名（这里是\"data\"）\n",
    "load_dataset(\"json\", data_files=\"./cmrc2018_trial.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义的加载脚本load_script.py加载数据集，并将其赋值给变量dataset。\n",
    "# 其中，split参数指定了要加载的数据集的划分方式（这里是\"train\"）\n",
    "dataset = load_dataset(\"./load_script.py\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、保存数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将处理后的数据集datasets保存到当前目录下的\"./processed_data\"文件夹中。\n",
    "datasets.save_to_disk(\"./processed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"madao33/new-title-chinese\")\n",
    "# 查看索引为1的数据\n",
    "datasets[\"train\"][1]\n",
    "# 查看数据集前2条数据\n",
    "datasets[\"train\"][:2]\n",
    "# 获取训练集中标题列的前5个元素\n",
    "datasets[\"train\"][\"title\"][:5]\n",
    "# 获取训练集的列名\n",
    "datasets[\"train\"].column_names\n",
    "# 获取训练集中的特征信息\n",
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[\"train\"]\n",
    "# 使用train_test_split方法将数据集划分为训练集和测试集，其中测试集占比为0.1\n",
    "dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用train_test_split方法将数据集划分为训练集和测试集，\n",
    "# 其中测试集占比为0.1，并按照\"label\"列进行分层抽样\n",
    "# 保证训练集和测试集中各类别的样本比例与原始数据集中的比例相同\n",
    "dataset.train_test_split(test_size=0.1, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取：选择训练集中索引为0和1的数据\n",
    "datasets[\"train\"].select([0, 1])\n",
    "\n",
    "# 过滤: 过滤训练集中标题包含\"中国\"的数据，获取过滤后的训练集的前5个标题\n",
    "filter_dataset = datasets[\"train\"].filter(lambda example: \"中国\" in example[\"title\"])\n",
    "filter_dataset[\"title\"][:5]\n",
    "\n",
    "# 映射: 通过函数进行数据集过滤\n",
    "def add_prefix(example):\n",
    "    example[\"title\"] = 'Prefix: ' + example[\"title\"]\n",
    "    return example\n",
    "prefix_dataset = datasets.map(add_prefix)\n",
    "prefix_dataset[\"train\"][:10][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def preprocess_function(example):\n",
    "    model_inputs = tokenizer(example[\"content\"], max_length=512, truncation=True)\n",
    "    labels = tokenizer(example[\"title\"], max_length=32, truncation=True)\n",
    "    # labels就是title编码的结果\n",
    "    # \"labels\"键是用来存储标题编码结果的。在训练模型时，我们通常需要指定输入的标签或目标，以便计算损失函数并进行模型优化\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "processed_datasets = datasets.map(preprocess_function)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用了4个进程并行地对数据集进行处理，以提高处理速度\n",
    "processed_datasets = datasets.map(preprocess_function, num_proc=4)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用了批处理的方式对数据集进行处理，可以处理较大的数据集\n",
    "processed_datasets = datasets.map(preprocess_function, batched=True)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用批处理的方式对数据集进行处理，并且移除了原始数据集中的列信息，只保留了处理后的数据集。\n",
    "processed_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、DataCollator\n",
    "DataCollatorWithPadding是Hugging Face Transformers库中的一个类，用于处理文本数据。它的主要功能和作用如下：\n",
    "\n",
    "- 填充（Padding）: DataCollatorWithPadding可以将不同长度的文本序列进行填充，使它们具有相同的长度。这对于批量输入到神经网络模型中非常重要，因为模型通常要求输入数据具有相同的形状。\n",
    "- 动态填充（Dynamic Padding）: DataCollatorWithPadding可以根据输入数据的最大长度动态地调整填充的长度。这意味着如果一个批次中的最长文本序列长度为100，那么所有其他较短的文本序列将被填充至长度为100。\n",
    "- 指定填充值（Padding Value）: DataCollatorWithPadding允许用户指定填充值，默认为0。这可以用于区分填充部分和实际文本内容。\n",
    "- 支持多种框架（Framework Support）: DataCollatorWithPadding支持多种深度学习框架，包括PyTorch、TensorFlow等。这使得用户可以在不同的框架之间灵活切换，而无需更改数据处理代码。<br>\n",
    "DataCollatorWithPadding是一个功能强大的工具，可以帮助用户在处理文本数据时进行填充操作，确保输入数据的一致性和模型训练的稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  DataCollatorWithPadding\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"/root/pretrains/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\", split='train')\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "tokenized_dataset = dataset.map(process_function, batched=True, remove_columns=dataset.column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、数据填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 创建DataLoader对象，指定批量大小为4，使用collator进行数据填充，并进行随机打乱\n",
    "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=collator, shuffle=True)\n",
    "#获取下一个批次的数据\n",
    "next(enumerate(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
