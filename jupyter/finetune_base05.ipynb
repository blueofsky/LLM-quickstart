{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大语言模型Transformer库-Model组件实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# 在线加载\n",
    "# model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True)\n",
    "# 离线加载\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"hfl/rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定保存模型和分词器的目录路径\n",
    "model_save_path = \"path_to_save_model\"\n",
    "tokenizer_save_path = \"path_to_save_tokenizer\"\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载参数\n",
    "```bash\n",
    "BertConfig {\n",
    "  \"_name_or_path\": \"/root/代码/Model组件/rbt3\",\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"directionality\": \"bidi\",\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 3,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"pooler_fc_size\": 768,\n",
    "  \"pooler_num_attention_heads\": 12,\n",
    "  \"pooler_num_fc_layers\": 3,\n",
    "  \"pooler_size_per_head\": 128,\n",
    "  \"pooler_type\": \"first_token_transform\",\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.2\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 21128\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"hfl/rbt3\")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"hfl/rbt3\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"今天天气不错，我的心情也不错！\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/代码/Model组件/rbt3\")\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不带Model Head的模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载时设置了output_attentions=True。\n",
    "# 这意味着模型在推理时不仅会输出最终的logits或预测结果，还会输出每个层的注意力权重。\n",
    "# 这通常用于调试或分析模型的内部工作机制\n",
    "model = AutoModel.from_pretrained(\"/root/代码/Model组件/rbt3\", output_attentions=True)\n",
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带Model Head的模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "# 使用AutoModelForSequenceClassification类加载一个用于序列分类任务的预训练模型。\n",
    "# 这个类是专门用于文本分类任务的，如情感分析或主题分类，它期望输出是整个输入序列的分类结果。\n",
    "# 它从指定路径加载模型，并使用inputs进行推理。\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"/root/代码/Model组件/rbt3\")\n",
    "\n",
    "clz_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码与上面代码类似，但它在加载模型时额外指定了num_labels=2参数。\n",
    "# 这个参数对于序列分类模型是重要的，因为它告诉模型输出层期望的输出尺寸，即分类标签的数量。\n",
    "# 如果模型被微调用于一个具有两个标签的分类任务（如正面情感和负面情感），这个参数是必须的。\n",
    "# 如果没有正确设置num_labels，模型的输出可能无法正确映射到标签空间。\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"/root/代码/Model组件/rbt3\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
