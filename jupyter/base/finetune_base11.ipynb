{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大语言模型预训练-微调技术之QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"llm-wizard/alpaca-gpt4-data-zh\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    # 设置最大长度为256\n",
    "    MAX_LENGTH = 256\n",
    "    # 初始化输入ID、注意力掩码和标签列表\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    # 对指令和输入进行编码\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    # 对输出进行编码，并添加结束符\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    # 将指令和响应的输入ID拼接起来\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    # 将指令和响应的注意力掩码拼接起来\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    # 将指令的标签设置为-100，表示不计算损失；将响应的输入ID作为标签\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    # 如果输入ID的长度超过最大长度，截断输入ID、注意力掩码和标签\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    # 返回处理后的数据\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "##修改\n",
    "# low_cpu_mem_usage=True: 这个参数设定为True意味着在模型加载时会尽可能地减少CPU内存的使用。\n",
    "# torch_dtype=torch.half: 这个参数设置了模型中张量的数据类型为半精度浮点数，这可以减少内存占用和计算时间，但可能会牺牲一些精度。\n",
    "# device_map=\"auto\": 这个参数设置了模型应该在哪个设备上运行。“auto”意味着它将自动选择可用的设备，优先选择GPU，如果没有GPU则选择CPU。\n",
    "# load_in_4bit=True: 这个参数设置为True意味着在模型加载时将使用4位量化，这可以进一步减少内存占用。\n",
    "# bnb_4bit_compute_dtype=torch.half: 这个参数设置了在4位量化时的计算数据类型，这里设置为半精度浮点数。\n",
    "# bnb_4bit_quant_type=\"nf4\": 这个参数设置了4位量化的类型，\"nf4\"是一种特定的量化策略。\n",
    "# bnb_4bit_use_double_quant=True: 这个参数设置为True意味着在4位量化时使用双重量化。\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\",\n",
    "                                              torch_dtype=torch.half,\n",
    "                                              low_cpu_mem_usage=True, \n",
    "                                              device_map=\"auto\", \n",
    "                                              load_in_4bit=True,\n",
    "                                              bnb_4bit_quant_type=\"nf4\", \n",
    "                                              bnb_4bit_use_double_quant=True)\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看参数，查看模型有哪些层，可以用于添加LoRA旁路\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(name,parameter.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、PEFT 步骤1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "## ,target_modules=[\"query_key_value\"],r=8\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM,r=8, target_modules=['query_key_value'])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在深度神经网络 [deep neural network] 训练时，需要对每个参数或权重 [parameter/weight] 计算其对损失函数 \n",
    "# [loss function] 的梯度 [gradient]，从而进行反向传播 [back propagation] 和优化[optimization]。\n",
    "# 默认情况下不会计算输入数据 [input data] 的梯度，即使它们在计算中起到了关键的作用。但是，在某些应用场景中，\n",
    "# 例如图像生成 [image generation]、注意力机制 [attention mechanism] 等，需要计算输入数据的梯度。此时，\n",
    "# 可以通过启用计算输入梯度的功能，对输入数据进行求导并利用其梯度信息进行优化。\n",
    "# 作用: 启用该功能这对于在保持模型权重固定的同时微调适配器权重非常有用。\n",
    "\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、PEFT 步骤2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看配置\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/cache/finetuning/bloom-1b4-zh-qlora\",  # 指定模型训练结果的输出目录\n",
    "    per_device_train_batch_size=4,  # 设置每个设备（如GPU）在训练过程中的批次大小为4\n",
    "    gradient_accumulation_steps=8,  # 指定梯度累积步数为8，即将多个批次的梯度累加后再进行一次参数更新\n",
    "    logging_steps=20,  # 每20个步骤记录一次日志信息\n",
    "    num_train_epochs=1,  # 指定训练的总轮数为1\n",
    "    gradient_checkpointing=True,  # 启用梯度检查点技术，可以减少内存占用并加速训练过程\n",
    "    optim=\"paged_adamw_32bit\"  # 指定分页优化器为\"paged_adamw_32bit\"，这是一种针对低秩模型的优化算法\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "#加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\", low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "\n",
    "#加载lora模型\n",
    "p_model = PeftModel.from_pretrained(model=model, model_id=\"/root/autodl-tmp/cache/finetuning/bloom-1b4-zh-qlora/checkpoint-500\")\n",
    "\n",
    "#模型推理\n",
    "pipe = pipeline(\"text-generation\", model=p_model, tokenizer=tokenizer, device=0)\n",
    "ipt = \"Human: {}\\n{}\".format(\"如何写好一个简历？\", \"\").strip() + \"\\n\\nAssistant: \"\n",
    "pipe(ipt, max_length=500, do_sample=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
